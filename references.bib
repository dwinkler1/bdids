
@misc{knausDynamicTripleGamma2023,
	title = {The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models},
	url = {http://arxiv.org/abs/2312.10487},
	doi = {10.48550/arXiv.2312.10487},
	abstract = {Many current approaches to shrinkage within the time-varying parameter framework assume that each state is equipped with only one innovation variance for all time points. Sparsity is then induced by shrinking this variance towards zero. We argue that this is not sufficient if the states display large jumps or structural changes, something which is often the case in time series analysis. To remedy this, we propose the dynamic triple gamma prior, a stochastic process that has a well-known triple gamma marginal form, while still allowing for autocorrelation. Crucially, the triple gamma has many interesting limiting and special cases (including the horseshoe shrinkage prior) which can also be chosen as the marginal distribution. Not only is the marginal form well understood, we further derive many interesting properties of the dynamic triple gamma, which showcase its dynamic shrinkage characteristics. We develop an efficient Markov chain Monte Carlo algorithm to sample from the posterior and demonstrate the performance through sparse covariance modeling and forecasting of the returns of the components of the {EURO} {STOXX} 50 index.},
	number = {{arXiv}:2312.10487},
	publisher = {{arXiv}},
	author = {Knaus, Peter and Frühwirth-Schnatter, Sylvia},
	urldate = {2023-12-22},
	date = {2023-12-16},
	eprinttype = {arxiv},
	eprint = {2312.10487 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/P6BNQ2UE/Knaus and Frühwirth-Schnatter - 2023 - The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/97WVPDQF/2312.html:text/html},
}


@article{fruhwirth-schnatterStochasticModelSpecification2010,
	title = {Stochastic model specification search for Gaussian and partial non-Gaussian state space models},
	volume = {154},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407609001614},
	doi = {10.1016/j.jeconom.2009.07.003},
	abstract = {Model specification for state space models is a difficult task as one has to decide which components to include in the model and to specify whether these components are fixed or time-varying. To this aim a new model space {MCMC} method is developed in this paper. It is based on extending the Bayesian variable selection approach which is usually applied to variable selection in regression models to state space models. For non-Gaussian state space models stochastic model search {MCMC} makes use of auxiliary mixture sampling. We focus on structural time series models including seasonal components, trend or intervention. The method is applied to various well-known time series.},
	pages = {85--100},
	number = {1},
	journaltitle = {Journal of Econometrics},
	shortjournal = {Journal of Econometrics},
	author = {Frühwirth-Schnatter, Sylvia and Wagner, Helga},
	urldate = {2024-01-19},
	date = {2010-01-01},
	keywords = {Auxiliary mixture sampling, Bayesian econometrics, Markov chain Monte Carlo, Non-centered parameterization, Variable selection},
	file = {ScienceDirect Snapshot:/Users/daniel/Zotero/storage/QGUG23PX/S0304407609001614.html:text/html;Submitted Version:/Users/daniel/Zotero/storage/A7QQNUPA/Frühwirth-Schnatter and Wagner - 2010 - Stochastic model specification search for Gaussian and partial non-Gaussian state space models.pdf:application/pdf},
}

@article{makowskiBayestestRDescribingEffects2019,
	title = {{bayestestR}: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework},
	volume = {4},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.01541},
	doi = {10.21105/joss.01541},
	shorttitle = {{bayestestR}},
	abstract = {Makowski et al., (2019). {bayestestR}: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541, https://doi.org/10.21105/joss.01541},
	pages = {1541},
	number = {40},
	journaltitle = {Journal of Open Source Software},
	author = {Makowski, Dominique and Ben-Shachar, Mattan S. and Lüdecke, Daniel},
	urldate = {2023-12-20},
	date = {2019-08-13},
	langid = {english},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/7BC2GKLV/Makowski et al. - 2019 - bayestestR Describing Effects and their Uncertainty, Existence and Significance within the Bayesian.pdf:application/pdf},
}

@misc{leeSimpleTransformationApproach2023,
	location = {Rochester, {NY}},
	title = {A Simple Transformation Approach to Difference-in-Differences Estimation for Panel Data},
	url = {https://papers.ssrn.com/abstract=4516518},
	doi = {10.2139/ssrn.4516518},
	abstract = {In the case of panel data, we propose a simple time-series transformation that can be combined with various treatment effect estimators, including regression adjustment, matching methods, and doubly robust estimators. The approach is motivated by the fact that, in the common timing case, our transformation, when applied with linear regression adjustment, numerically reproduces the pooled {OLS} estimator in Wooldridge (2021). In the general staggered case, the transformation is at the unit level, and simply requires computing the average outcome prior to an intervention, subtracting it from a post-treatment outcome, and then carefully selecting the control units in each time period. We show formally that, allowing for staggered entry under no anticipation and parallel trends assumptions, the cohort treatment indicators satisfy the key unconfoundedness assumption with respect to the transformed potential outcome. Given identification, any number of treatment effect estimators can be applied for each treated cohort and calendar time pair where the average treatment effects on the treated are identified. In effect, we establish the consistency of intuitively appealing rolling methods. The doubly robust method of combining inverse probability weighting with linear regression works particularly well in terms of bias and efficiency. Long differencing methods, such as those proposed by Callaway and Sant'Anna (2021), can be considerably less efficient. We also show how to modify the transformation to account for unit-specific trends.},
	number = {4516518},
	author = {Lee, Soo Jeong and Wooldridge, Jeffrey M.},
	urldate = {2024-01-03},
	date = {2023-07-20},
	langid = {english},
	keywords = {panel data, Difference-in-differences, doubly robust estimators, heterogenous trends, parallel trends},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/QAUNQHQD/Lee and Wooldridge - 2023 - A Simple Transformation Approach to Difference-in-Differences Estimation for Panel Data.pdf:application/pdf},
}

@article{borusyakRevisitingEventStudy,
	title = {Revisiting Event Study Designs: Robust and Efficient Estimation},
	abstract = {A broad empirical literature uses “event study,” or “diﬀerence-in-diﬀerences with staggered rollout,” research designs for treatment eﬀect estimation: settings in which units in the panel receive treatment at diﬀerent times. We show a series of problems with conventional regression-based two-way ﬁxed eﬀects estimators, both static and dynamic. These problems arise when researchers conﬂate the identifying assumptions of parallel trends and no anticipatory eﬀects, implicit assumptions that restrict treatment eﬀect heterogeneity, and the speciﬁcation of the estimand as a weighted average of treatment eﬀects. We then derive the eﬃcient estimator robust to treatment eﬀect heterogeneity for this setting, show that it has a particularly intuitive “imputation” form when treatment-eﬀect heterogeneity is unrestricted, characterize its asymptotic behavior, provide tools for inference, and illustrate its attractive properties in simulations. We further discuss appropriate tests for parallel trends, and show how our estimation approach extends to many settings beyond standard event studies.},
	author = {Borusyak, Kirill},
	langid = {english},
	file = {Borusyak - Revisiting Event Study Designs Robust and Efficie.pdf:/Users/daniel/Zotero/storage/FBQWVZHT/Borusyak - Revisiting Event Study Designs Robust and Efficie.pdf:application/pdf},
}

@article{rambachanMoreCredibleApproach2023a,
	title = {A More Credible Approach to Parallel Trends},
	volume = {90},
	issn = {0034-6527, 1467-937X},
	url = {https://academic.oup.com/restud/article/90/5/2555/7039335},
	doi = {10.1093/restud/rdad018},
	abstract = {This paper proposes tools for robust inference in diﬀerence-in-diﬀerences and eventstudy designs where the parallel trends assumption may be violated. Instead of requiring that parallel trends holds exactly, we impose restrictions on how diﬀerent the post-treatment violations of parallel trends can be from the pre-treatment diﬀerences in trends (“pre-trends”). The causal parameter of interest is partially identiﬁed under these restrictions. We introduce two approaches that guarantee uniformly valid inference under the imposed restrictions, and we derive novel results showing that they have desirable power properties in our context. We illustrate how economic knowledge can inform the restrictions on the possible violations of parallel trends in two economic applications. We also highlight how our approach can be used to conduct sensitivity analyses showing what causal conclusions can be drawn under various restrictions on the possible violations of the parallel trends assumption.},
	pages = {2555--2591},
	number = {5},
	journaltitle = {Review of Economic Studies},
	author = {Rambachan, Ashesh and Roth, Jonathan},
	urldate = {2023-12-20},
	date = {2023-09-05},
	langid = {english},
	file = {Rambachan and Roth - 2023 - A More Credible Approach to Parallel Trends.pdf:/Users/daniel/Zotero/storage/7MVLVDYI/Rambachan and Roth - 2023 - A More Credible Approach to Parallel Trends.pdf:application/pdf},
}

@misc{knausDynamicTripleGamma2023,
	title = {The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models},
	url = {http://arxiv.org/abs/2312.10487},
	doi = {10.48550/arXiv.2312.10487},
	abstract = {Many current approaches to shrinkage within the time-varying parameter framework assume that each state is equipped with only one innovation variance for all time points. Sparsity is then induced by shrinking this variance towards zero. We argue that this is not sufficient if the states display large jumps or structural changes, something which is often the case in time series analysis. To remedy this, we propose the dynamic triple gamma prior, a stochastic process that has a well-known triple gamma marginal form, while still allowing for autocorrelation. Crucially, the triple gamma has many interesting limiting and special cases (including the horseshoe shrinkage prior) which can also be chosen as the marginal distribution. Not only is the marginal form well understood, we further derive many interesting properties of the dynamic triple gamma, which showcase its dynamic shrinkage characteristics. We develop an efficient Markov chain Monte Carlo algorithm to sample from the posterior and demonstrate the performance through sparse covariance modeling and forecasting of the returns of the components of the {EURO} {STOXX} 50 index.},
	number = {{arXiv}:2312.10487},
	publisher = {{arXiv}},
	author = {Knaus, Peter and Frühwirth-Schnatter, Sylvia},
	urldate = {2023-12-22},
	date = {2023-12-16},
	eprinttype = {arxiv},
	eprint = {2312.10487 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/P6BNQ2UE/Knaus and Frühwirth-Schnatter - 2023 - The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/97WVPDQF/2312.html:text/html},
}


@article{cadonnaTripleGammaMdash2020,
	title = {Triple the Gamma\&mdash;A Unifying Shrinkage Prior for Variance and Variable Selection in Sparse State Space and {TVP} Models},
	volume = {8},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {22251146},
	url = {https://sciprofiles.com/publication/view/bcd8e49228e12dcab96a4e86b61f99eb},
	doi = {10.3390/econometrics8020020},
	abstract = {Time-varying parameter ({TVP}) models are very flexible in capturing gradual changes in the effect of explanatory variables on the outcome variable. However, in particular when the number of explanatory variables is large, there is a known risk of overfitting and poor predictive performance, since the effect of some explanatory variables is constant over time. We propose a new prior for variance shrinkage in {TVP} models, called triple gamma. The triple gamma prior encompasses a number of priors that have been suggested previously, such as the Bayesian Lasso, the double gamma prior and the Horseshoe prior. We present the desirable properties of such a prior and its relationship to Bayesian Model Averaging for variance selection. The features of the triple gamma prior are then illustrated in the context of time varying parameter vector autoregressive models, both for simulated dataset and for a series of macroeconomics variables in the Euro Area.},
	pages = {20},
	number = {2},
	journaltitle = {Econometrics},
	author = {Cadonna, Annalisa and Frühwirth-Schnatter, Sylvia and Knaus, Peter},
	urldate = {2023-12-22},
	date = {2020-05-20},
	langid = {english},
	note = {Number: 2
Publisher: {MDPI} {AG}},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/YUCIWBA8/Cadonna et al. - 2020 - Triple the Gamma&mdash\;A Unifying Shrinkage Prior for Variance and Variable Selection in Sparse Stat.pdf:application/pdf},
}


@article{mccauslandSimulationSmoothingState2011,
	title = {Simulation smoothing for state–space models: A computational efficiency analysis},
	volume = {55},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947310002823},
	doi = {10.1016/j.csda.2010.07.009},
	shorttitle = {Simulation smoothing for state–space models},
	abstract = {Simulation smoothing involves drawing state variables (or innovations) in discrete time state–space models from their conditional distribution given parameters and observations. Gaussian simulation smoothing is of particular interest, not only for the direct analysis of Gaussian linear models, but also for the indirect analysis of more general models. Several methods for Gaussian simulation smoothing exist, most of which are based on the Kalman filter. Since states in Gaussian linear state–space models are Gaussian Markov random fields, it is also possible to apply the Cholesky Factor Algorithm ({CFA}) to draw states. This algorithm takes advantage of the band diagonal structure of the Hessian matrix of the log density to make efficient draws. We show how to exploit the special structure of state–space models to draw latent states even more efficiently. We analyse the computational efficiency of Kalman-filter-based methods, the {CFA}, and our new method using counts of operations and computational experiments. We show that for many important cases, our method is most efficient. Gains are particularly large for cases where the dimension of observed variables is large or where one makes repeated draws of states for the same parameter values. We apply our method to a multivariate Poisson model with time-varying intensities, which we use to analyse financial market transaction count data. © 2010 Elsevier B.V. All rights reserved.},
	pages = {199--212},
	number = {1},
	journaltitle = {Computational Statistics \& Data Analysis},
	author = {{McCausland}, William J. and Miller, Shirley and Pelletier, Denis},
	urldate = {2019-04-18},
	date = {2011-01},
	langid = {english},
	file = {McCausland et al. - 2011 - Simulation smoothing for state–space models A com.pdf:/Users/daniel/Zotero/storage/823EMLV7/McCausland et al. - 2011 - Simulation smoothing for state–space models A com.pdf:application/pdf},
}


@article{dickeyWeightedLikelihoodRatio1970,
	title = {The Weighted Likelihood Ratio, Sharp Hypotheses about Chances, the Order of a Markov Chain},
	volume = {41},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-41/issue-1/The-Weighted-Likelihood-Ratio-Sharp-Hypotheses-about-Chances-the-Order/10.1214/aoms/1177697203.full},
	doi = {10.1214/aoms/1177697203},
	abstract = {The Bayesian theory for testing a sharp hypothesis, defined by fixed values of parameters, is here presented in general terms. Arbitrary positive prior probability is attached to the hypothesis. The ratio of posterior to prior odds for the hypothesis is given by the weighted likelihood ratio, shown here to equal Leonard J. Savage's (1963) ratio of a posterior to a prior density (2.21). This Bayesian approach to hypothesis testing was suggested by Jeffreys (1948), Savage (1959), (1961), Lindley (1961), and Good (1950), (1965), but obscured some what by approximations and unique choices of prior distributions. This Bayesian theory is distinct from that of Lindley (1965) and that of Dickey (1967a). Applications are given to hypotheses about multinomial means, for example, equality of two binomial probabilities. A new test is presented for the order of a finite-state Markov chain.},
	pages = {214--226},
	number = {1},
	journaltitle = {The Annals of Mathematical Statistics},
	author = {Dickey, James M. and Lientz, B. P.},
	urldate = {2023-12-20},
	date = {1970-02},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/GBENEEDC/Dickey and Lientz - 1970 - The Weighted Likelihood Ratio, Sharp Hypotheses about Chances, the Order of a Markov Chain.pdf:application/pdf},
}

@article{wagenmakersBayesianHypothesisTesting2010a,
	title = {Bayesian hypothesis testing for psychologists: A tutorial on the Savage–Dickey method},
	volume = {60},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028509000826},
	doi = {10.1016/j.cogpsych.2009.12.001},
	shorttitle = {Bayesian hypothesis testing for psychologists},
	abstract = {In the ﬁeld of cognitive psychology, the p-value hypothesis test has established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage–Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method’s validity, generality, and ﬂexibility.},
	pages = {158--189},
	number = {3},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
	urldate = {2023-12-20},
	date = {2010-05},
	langid = {english},
	file = {Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A tutorial on the Savage–Dickey method.pdf:/Users/daniel/Zotero/storage/K9N3CHPU/Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A tutorial on the Savage–Dickey method.pdf:application/pdf},
}



@article{altmanStatisticsNotesAbsence1995,
	title = {Statistics notes: Absence of evidence is not evidence of absence},
	volume = {311},
	rights = {© 1995 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/311/7003/485},
	doi = {10.1136/bmj.311.7003.485},
	shorttitle = {Statistics notes},
	abstract = {The non-equivalence of statistical significance and clinical importance has long been recognised, but this error of interpretation remains common. Although a significant result in a large study may sometimes not be clinically important, a far greater problem arises from misinterpretation of non-significant findings. By convention a P value greater than 5\% (P{\textgreater}0.05) is called “not significant.” Randomised controlled clinical trials that do not show a significant difference between the treatments being compared are often called “negative.” This term wrongly implies that the study has shown that there is no difference, whereas usually all that has been shown is an absence of evidence of a difference. These are quite different statements.

The sample size of controlled trials is generally inadequate, with a consequent lack of power to detect real, and clinically worthwhile, differences in treatment. Freiman et al1 found that only …},
	pages = {485},
	number = {7003},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, Douglas G. and Bland, J. Martin},
	urldate = {2023-12-19},
	date = {1995-08-19},
	langid = {english},
	pmid = {7647644},
	note = {Publisher: British Medical Journal Publishing Group
Section: Paper},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/NZ8UID99/Altman and Bland - 1995 - Statistics notes Absence of evidence is not evidence of absence.pdf:application/pdf},
}

@article{wassersteinMovingWorld052019,
	title = {Moving to a World Beyond “p {\textless} 0.05”},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2019.1583913},
	doi = {10.1080/00031305.2019.1583913},
	pages = {1--19},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
	urldate = {2023-12-19},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2019.1583913},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/ZWT6KXUD/Wasserstein et al. - 2019 - Moving to a World Beyond “p  0.05”.pdf:application/pdf},
}


@article{callawayDifferenceinDifferencesMultipleTime2021a,
	title = {Difference-in-Differences with multiple time periods},
	volume = {225},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407620303948},
	doi = {10.1016/j.jeconom.2020.12.001},
	series = {Themed Issue: Treatment Effect 1},
	abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences ({DiD}) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered {DiD} setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.},
	pages = {200--230},
	number = {2},
	journaltitle = {Journal of Econometrics},
	shortjournal = {Journal of Econometrics},
	author = {Callaway, Brantly and Sant’Anna, Pedro H. C.},
	urldate = {2023-12-19},
	date = {2021-12-01},
	keywords = {Difference-in-Differences, Doubly robust, Dynamic treatment effects, Event study, Semi-parametric, Treatment effect heterogeneity, Variation in treatment timing},
	file = {ScienceDirect Snapshot:/Users/daniel/Zotero/storage/9GVRIEYH/S0304407620303948.html:text/html;Submitted Version:/Users/daniel/Zotero/storage/4ZWC9KPU/Callaway and Sant’Anna - 2021 - Difference-in-Differences with multiple time periods.pdf:application/pdf},
}

@book{cameron2005microeconometrics,
  title={Microeconometrics: methods and applications},
  author={Cameron, A Colin and Trivedi, Pravin K},
  year={2005},
  publisher={Cambridge university press}
}

@article{goodman-baconDifferenceindifferencesVariationTreatment2021c,
	title = {Difference-in-differences with variation in treatment timing},
	volume = {225},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407621001445},
	doi = {10.1016/j.jeconom.2021.03.014},
	series = {Themed Issue: Treatment Effect 1},
	abstract = {The canonical difference-in-differences ({DD}) estimator contains two time periods, ”pre” and ”post”, and two groups, ”treatment” and ”control”. Most {DD} applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period {DD} estimators in the data. A causal interpretation of two-way fixed effects {DD} estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
	pages = {254--277},
	number = {2},
	journaltitle = {Journal of Econometrics},
	shortjournal = {Journal of Econometrics},
	author = {Goodman-Bacon, Andrew},
	urldate = {2023-12-19},
	date = {2021-12-01},
	keywords = {Difference-in-differences, Treatment effect heterogeneity, Two-way fixed effects, Variation in treatment timing},
	file = {ScienceDirect Full Text PDF:/Users/daniel/Zotero/storage/QQCNRX6L/Goodman-Bacon - 2021 - Difference-in-differences with variation in treatment timing.pdf:application/pdf;ScienceDirect Snapshot:/Users/daniel/Zotero/storage/34GT3TBC/S0304407621001445.html:text/html},
}

@article{rubinCausalInferenceUsing2005,
	title = {Causal Inference Using Potential Outcomes: Design, Modeling, Decisions},
	volume = {100},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/27590541},
	shorttitle = {Causal Inference Using Potential Outcomes},
	abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
	pages = {322--331},
	number = {469},
	journaltitle = {Journal of the American Statistical Association},
	author = {Rubin, Donald B.},
	urldate = {2023-12-18},
	date = {2005},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	file = {JSTOR Full Text PDF:/Users/daniel/Zotero/storage/JBXR5YJX/Rubin - 2005 - Causal Inference Using Potential Outcomes Design, Modeling, Decisions.pdf:application/pdf},
}

@article{sunEstimatingDynamicTreatment2021a,
	title = {Estimating dynamic treatment effects in event studies with heterogeneous treatment effects},
	volume = {225},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S030440762030378X},
	doi = {10.1016/j.jeconom.2020.09.006},
	series = {Themed Issue: Treatment Effect 1},
	abstract = {To estimate the dynamic effects of an absorbing treatment, researchers often use two-way fixed effects regressions that include leads and lags of the treatment. We show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. We propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.},
	pages = {175--199},
	number = {2},
	journaltitle = {Journal of Econometrics},
	shortjournal = {Journal of Econometrics},
	author = {Sun, Liyang and Abraham, Sarah},
	urldate = {2023-12-19},
	date = {2021-12-01},
	keywords = {Difference-in-differences, Pretrend test, Two-way fixed effects},
	file = {ScienceDirect Full Text PDF:/Users/daniel/Zotero/storage/BPVRHGZL/Sun and Abraham - 2021 - Estimating dynamic treatment effects in event studies with heterogeneous treatment effects.pdf:application/pdf},
}
